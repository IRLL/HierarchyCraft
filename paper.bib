@inproceedings{johnson2016malmo,
  title={The Malmo Platform for Artificial Intelligence Experimentation},
  author={Johnson, Matthew and Hofmann, Katja and Hutton, Tim and Bignell, David},
  booktitle={IJCAI},
  pages={4246--4247},
  year={2016}
}

@misc{UPF,
  title={Unified Planning Framework},
  url={https://github.com/aiplan4eu/unified-planning},
  publisher={GitHub},
  journal={GitHub repository},
  year={2023}
}

@inproceedings{hafner2022benchmarking,
  title={Benchmarking the Spectrum of Agent Capabilities},
  author={Danijar Hafner},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=1W0z96MFEoH}
}


@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}


@inproceedings{minigrid,
  author       = {Maxime Chevalier{-}Boisvert and Bolun Dai and Mark Towers and Rodrigo Perez{-}Vicente and Lucas Willems and Salem Lahlou and Suman Pal and Pablo Samuel Castro and Jordan Terry},
  title        = {Minigrid {\&} Miniworld: Modular {\&} Customizable Reinforcement Learning Environments for Goal-Oriented Tasks},
  booktitle    = {Advances in Neural Information Processing Systems 36, New Orleans, LA, USA},
  month        = {December},
  year         = {2023},
  url          = {https://github.com/Farama-Foundation/Minigrid},
}

@article{machado2018revisiting,
  title={Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents},
  author={Machado, Marlos C and Bellemare, Marc G and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={523--562},
  year={2018}
}

@article{kuttler2020nethack,
  title={The NetHack learning environment},
  author={K{\"u}ttler, Heinrich and Nardelli, Nantas and Miller, Alexander and Raileanu, Roberta and Selvatici, Marco and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7671--7684},
  year={2020}
}

@InProceedings{milani2020minerl2019,
  title = 	 {Retrospective Analysis of the 2019 MineRL Competition on Sample Efficient Reinforcement Learning},
  author =       {Milani, Stephanie and Topin, Nicholay and Houghton, Brandon and Guss, William H. and Mohanty, Sharada P. and Nakata, Keisuke and Vinyals, Oriol and Kuno, Noboru Sean},
  booktitle = 	 {Proceedings of the NeurIPS 2019 Competition and Demonstration Track},
  pages = 	 {203--214},
  year = 	 {2020},
  editor = 	 {Escalante, Hugo Jair and Hadsell, Raia},
  volume = 	 {123},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {08--14 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v123/milani20a/milani20a.pdf},
  url = 	 {https://proceedings.mlr.press/v123/milani20a.html},
  abstract = 	 {To facilitate research in the direction of sample efficient reinforcement learning, we held the MineRL Competition on Sample Efficient Reinforcement Learning Using Human Priors at the Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019). The primary goal of this competition was to promote the development of algorithms that use human demonstrations alongside reinforcement learning to reduce the number of samples needed to solve complex, hierarchical, and sparse environments. We describe the competition, outlining the primary challenge, the competition design, and the resources that we provided to the participants. We provide an overview of the top solutions, each of which use deep reinforcement learning and/or imitation learning. We also discuss the impact of our organizational decisions on the competition and future directions for improvement.}
}

@article{guss2021minerl2020,
  author       = {William H. Guss and
                  Mario Ynocente Castro and
                  Sam Devlin and
                  Brandon Houghton and
                  Noboru Sean Kuno and
                  Crissman Loomis and
                  Stephanie Milani and
                  Sharada P. Mohanty and
                  Keisuke Nakata and
                  Ruslan Salakhutdinov and
                  John Schulman and
                  Shinya Shiroshita and
                  Nicholay Topin and
                  Avinash Ummadisingu and
                  Oriol Vinyals},
  title        = {The MineRL 2020 Competition on Sample Efficient Reinforcement Learning
                  using Human Priors},
  journal      = {CoRR},
  volume       = {abs/2101.11071},
  year         = {2021},
  url          = {https://arxiv.org/abs/2101.11071},
  eprinttype    = {arXiv},
  eprint       = {2101.11071},
  timestamp    = {Sun, 31 Jan 2021 17:23:50 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2101-11071.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{dreamerv3,
  title={Mastering Diverse Domains through World Models},
  author={Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:2301.04104},
  year={2023}
}

@inproceedings{procgen,
  title={Leveraging procedural generation to benchmark reinforcement learning},
  author={Cobbe, Karl and Hesse, Chris and Hilton, Jacob and Schulman, John},
  booktitle={International conference on machine learning},
  pages={2048--2056},
  year={2020},
  organization={PMLR}
}

@article{ALE,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}

@article{deepmindlab,
  author = {Charles Beattie and
                  Joel Z. Leibo and
                  Denis Teplyashin and
                  Tom Ward and
                  Marcus Wainwright and
                  Heinrich K{\"{u}}ttler and
                  Andrew Lefrancq and
                  Simon Green and
                  V{\'{\i}}ctor Vald{\'{e}}s and
                  Amir Sadik and
                  Julian Schrittwieser and
                  Keith Anderson and
                  Sarah York and
                  Max Cant and
                  Adam Cain and
                  Adrian Bolton and
                  Stephen Gaffney and
                  Helen King and
                  Demis Hassabis and
                  Shane Legg and
                  Stig Petersen},
  title        = {DeepMind Lab},
  journal      = {CoRR},
  volume       = {abs/1612.03801},
  year         = {2016},
  url          = {http://arxiv.org/abs/1612.03801},
  eprinttype    = {arXiv},
  eprint       = {1612.03801},
  timestamp    = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BeattieLTWWKLGV16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{gymnasium,
  title={Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author={Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U and De Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and others},
  journal={arXiv preprint arXiv:2407.17032},
  year={2024}
}

@article{PDDLgym,
  publtype={informal},
  author={Tom Silver and Rohan Chitnis},
  title={PDDLGym: Gym Environments from PDDL Problems},
  year={2020},
  cdate={1577836800000},
  journal={CoRR},
  volume={abs/2002.06432},
  url={https://arxiv.org/abs/2002.06432}
}


@article{PDDL,
  title={PDDL - The Planning Domain Definition Language},
  author={Drew McDermott, },
  journal={Artificial Intelligence Planning Systems, TR98003/DCS TR1165},
  year={1998}
}

@incollection{ENSHP,
  title={Interval-based relaxation for general numeric planning},
  author={Scala, Enrico and Haslum, Patrik and Thi{\'e}baux, Sylvie and Ramirez, Miquel},
  booktitle={ECAI 2016},
  pages={655--663},
  year={2016},
  publisher={IOS Press}
}
@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@inproceedings{ANML,
  title={The ANML Language},
  author={David E. Smith and Jeremy Frank and William Cushing},
  year={2007},
  url={https://api.semanticscholar.org/CorpusID:14116191}
}


@InProceedings{2021NetHack,
  title = 	 {Insights From the NeurIPS 2021 NetHack Challenge},
  author =       {Hambro, Eric and Mohanty, Sharada and Babaev, Dmitrii and Byeon, Minwoo and Chakraborty, Dipam and Grefenstette, Edward and Jiang, Minqi and Daejin, Jo and Kanervisto, Anssi and Kim, Jongmin and Kim, Sungwoong and Kirk, Robert and Kurin, Vitaly and K{\"u}ttler, Heinrich and Kwon, Taehwon and Lee, Donghoon and Mella, Vegard and Nardelli, Nantas and Nazarov, Ivan and Ovsov, Nikita and Holder, Jack and Raileanu, Roberta and Ramanauskas, Karolis and Rockt{\"a}schel, Tim and Rothermel, Danielle and Samvelyan, Mikayel and Sorokin, Dmitry and Sypetkowski, Maciej and Sypetkowski, Micha\l{}},
  booktitle = 	 {Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track},
  pages = 	 {41--52},
  year = 	 {2022},
  editor = 	 {Kiela, Douwe and Ciccone, Marco and Caputo, Barbara},
  volume = 	 {176},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--14 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v176/hambro22a/hambro22a.pdf},
  url = 	 {https://proceedings.mlr.press/v176/hambro22a.html},
  abstract = 	 {In this report, we summarize the takeaways from the first NeurIPS 2021 NetHack Challenge. Participants were tasked with developing a program or agent that can win (i.e., ’ascend’ in) the popular dungeon-crawler game of NetHack by interacting with the NetHack Learning Environment (NLE), a scalable, procedurally generated, and challenging Gym environment for reinforcement learning (RL). The challenge showcased community-driven progress in AI with many diverse approaches significantly beating the previously best results on NetHack. Furthermore, it served as a direct comparison between neural (e.g., deep RL) and symbolic AI, as well as hybrid systems, demonstrating that on NetHack symbolic bots currently outperform deep RL by a large margin. Lastly, no agent got close to winning the game, illustrating NetHack’s suitability as a long-term benchmark for AI research.}
}

@article{botvinick2014model,
  title={Model-based hierarchical reinforcement learning and human action control},
  author={Botvinick, Matthew and Weinstein, Aaron},
  journal={Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume={369},
  number={1655},
  pages={20130480},
  year={2014},
  publisher={The Royal Society}
}

@inproceedings{bacon2017option,
  author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  title = {The option-critic architecture},
  year = {2017},
  publisher = {AAAI Press},
  abstract = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup \& Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
  booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
  pages = {1726–1734},
  numpages = {9},
  location = {San Francisco, California, USA},
  series = {AAAI'17}
}

@inproceedings{heess2016learning,
  title={Learning continuous control policies by stochastic value gradients},
  author={Heess, Nicolas and Wayne, Greg and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={29},
  pages={2944--2952},
  year={2016}
}
@inproceedings{nachum2018data,
  title={Data-efficient hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={31},
  pages={3303--3313},
  year={2018}
}

@article{Chollet2019OnTM,
  title={On the Measure of Intelligence},
  author={François Chollet},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.01547},
  url={https://api.semanticscholar.org/CorpusID:207870692}
}
