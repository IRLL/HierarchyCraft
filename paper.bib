@article{UPF,
  title = {{Unified} {Planning}: Modeling, manipulating and solving {AI} planning problems in {Python}},
  author = {Andrea Micheli and Arthur Bit-Monnot and Gabriele R{\"o}ger and Enrico Scala and Alessandro Valentini and Luca Framba and Alberto Rovetta and Alessandro Trapasso and Luigi Bonassi and Alfonso Emilio Gerevini and Luca Iocchi and Felix Ingrand and Uwe Köckemann and Fabio Patrizi and Alessandro Saetti and Ivan Serina and Sebastian Stock},
  journal = {SoftwareX},
  volume = {29},
  pages = {102012},
  year = {2025},
  issn = {2352-7110},
  doi = {10.1016/j.softx.2024.102012}
}

@inproceedings{hafner2022benchmarking,
  title={Benchmarking the Spectrum of Agent Capabilities},
  author={Danijar Hafner},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=1W0z96MFEoH},
  doi={10.48550/arXiv.2109.06780}
}


@article{10.1016/s0004-37029900052-1,
  title={Between {MDPs} and semi-{MDPs}: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S. and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1--2},
  pages={181--211},
  year={1999},
  doi={10.1016/S0004-3702(99)00052-1},
}


@inproceedings{MinigridMiniworld23,
  author       = {Maxime Chevalier{-}Boisvert and Bolun Dai and Mark Towers and Rodrigo Perez{-}Vicente and Lucas Willems and Salem Lahlou and Suman Pal and Pablo Samuel Castro and Jordan Terry},
  editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  title        = {Minigrid {\&} {Miniworld}: Modular {\&} Customizable Reinforcement Learning Environments for Goal-Oriented Tasks},
  booktitle    = {Advances in Neural Information Processing Systems},
  isbn         = {978-1-71389-992-1},
  pages        = {73383--73394},
  publisher    = {Curran Associates, Inc.},
  month        = dec,
  year         = {2023},
  volume       = 36,
  doi = {10.48550/arXiv.2306.13831},
}

@article{kuttler2020nethack,
  title={The NetHack learning environment},
  author={K{\"u}ttler, Heinrich and Nardelli, Nantas and Miller, Alexander and Raileanu, Roberta and Selvatici, Marco and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7671--7684},
  year={2020},
  doi = {10.48550/arxiv.2006.13760},
}

@InProceedings{milani2020minerl2019,
  title = 	 {Retrospective Analysis of the 2019 {MineRL} Competition on Sample Efficient Reinforcement Learning},
  author =       {Milani, Stephanie and Topin, Nicholay and Houghton, Brandon and Guss, William H. and Mohanty, Sharada P. and Nakata, Keisuke and Vinyals, Oriol and Kuno, Noboru Sean},
  booktitle = 	 {Proceedings of the NeurIPS 2019 Competition and Demonstration Track},
  pages = 	 {203--214},
  year = 	 {2020},
  editor = 	 {Escalante, Hugo Jair and Hadsell, Raia},
  volume = 	 {123},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 dec,
  pdf = 	 {http://proceedings.mlr.press/v123/milani20a/milani20a.pdf},
  url = 	 {https://proceedings.mlr.press/v123/milani20a.html},
  abstract = 	 {To facilitate research in the direction of sample efficient reinforcement learning, we held the MineRL Competition on Sample Efficient Reinforcement Learning Using Human Priors at the Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019). The primary goal of this competition was to promote the development of algorithms that use human demonstrations alongside reinforcement learning to reduce the number of samples needed to solve complex, hierarchical, and sparse environments. We describe the competition, outlining the primary challenge, the competition design, and the resources that we provided to the participants. We provide an overview of the top solutions, each of which use deep reinforcement learning and/or imitation learning. We also discuss the impact of our organizational decisions on the competition and future directions for improvement.},
  doi = {10.48550/arxiv.2003.05012}
}

@article{guss2021minerl2020,
  author       = {William H. Guss and
                  Mario Ynocente Castro and
                  Sam Devlin and
                  Brandon Houghton and
                  Noboru Sean Kuno and
                  Crissman Loomis and
                  Stephanie Milani and
                  Sharada P. Mohanty and
                  Keisuke Nakata and
                  Ruslan Salakhutdinov and
                  John Schulman and
                  Shinya Shiroshita and
                  Nicholay Topin and
                  Avinash Ummadisingu and
                  Oriol Vinyals},
  title        = {The MineRL 2020 Competition on Sample Efficient Reinforcement Learning
                  using Human Priors},
  journal      = {CoRR},
  volume       = {abs/2101.11071},
  year         = {2021},
  doi          = {10.48550/arXiv.2101.11071},
  eprinttype    = {arXiv},
  eprint       = {2101.11071},
  timestamp    = {Sun, 31 Jan 2021 17:23:50 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2101-11071.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{dreamerv3,
  title={Mastering Diverse Domains through World Models},
  author={Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:2301.04104},
  year={2023},
  doi={10.48550/arXiv.2301.04104},
}

@inproceedings{procgen,
  title={Leveraging procedural generation to benchmark reinforcement learning},
  author={Cobbe, Karl and Hesse, Chris and Hilton, Jacob and Schulman, John},
  booktitle={International conference on machine learning},
  pages={2048--2056},
  year={2020},
  organization={PMLR},
  doi={10.48550/arxiv.1912.01588}
}

@inproceedings{10.5555/2832747.2832830,
  author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  title = {The {Arcade} {Learning} {Environment}: an evaluation platform for general agents},
  year = {2015},
  isbn = {978-1-57735-738-4},
  publisher = {AAAI Press},
  abstract = {In this extended abstract we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by presenting a benchmark set of domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. We conclude with a brief update on the latest ALE developments. All of the software, including the benchmark agents, is publicly available.},
  booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
  pages = {4148–4152},
  numpages = {5},
  location = {Buenos Aires, Argentina},
  series = {IJCAI'15},
  doi={10.1613/jair.3912}
}

@article{deepmindlab,
  author = {Charles Beattie and
                  Joel Z. Leibo and
                  Denis Teplyashin and
                  Tom Ward and
                  Marcus Wainwright and
                  Heinrich K{\"{u}}ttler and
                  Andrew Lefrancq and
                  Simon Green and
                  V{\'{\i}}ctor Vald{\'{e}}s and
                  Amir Sadik and
                  Julian Schrittwieser and
                  Keith Anderson and
                  Sarah York and
                  Max Cant and
                  Adam Cain and
                  Adrian Bolton and
                  Stephen Gaffney and
                  Helen King and
                  Demis Hassabis and
                  Shane Legg and
                  Stig Petersen},
  title        = {DeepMind Lab},
  journal      = {CoRR},
  volume       = {abs/1612.03801},
  year         = {2016},
  url          = {http://arxiv.org/abs/1612.03801},
  eprinttype    = {arXiv},
  eprint       = {1612.03801},
  timestamp    = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BeattieLTWWKLGV16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  doi={10.48550/arXiv.1612.03801},
}


@article{gymnasium,
  title={Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author={Mark Towers and Ariel Kwiatkowski and Jordan Terry and John U. Balis and Gianluca De Cola and Tristan Deleu and Manuel Goulão and Andreas Kallinteris and Markus Krimmel and Arjun KG and Rodrigo Perez-Vicente and Andrea Pierré and Sander Schulhoff and Jun Jet Tai and Hannah Tan and Omar G. Younis},
  journal={arXiv preprint arXiv:2407.17032},
  year={2024},
  doi={10.48550/arXiv.2407.17032},
}

@inproceedings{PDDLgym,
  author={Tom Silver and Rohan Chitnis},
  title={{PDDLGym}: {Gym} Environments from {PDDL} Problems},
  booktitle={Proceedings of the 1st Workshop on Bridging the Gap Between AI Planning and Reinforcement Learning},
  editor={Alan Fern and Vicenç Gómez and Anders Jonsson and Michael Katz and Hector Palacios and Scott Sanner},
  year={2020},
  pages={1--6},
  doi={10.48550/arxiv.2002.06432}
}

@techreport{PDDL,
  author       = {Malik Ghallab and Adele Howe and Craig Knoblock and Drew McDermott and Ashwin Ram and Manuela Veloso and Daniel Weld and David Wilkins},
  title        = {{PDDL}~-- the {Planning} {Domain} {Definition} {Language}, Version 1.2},
  institution  = {Yale Center for Computational Vision {and} Control},
  year         = 1998,
  type         = {Tech Report},
  number       = {CVC TR-98-003{\slash}DCS TR-1165},
  month        = oct,
}

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@inproceedings{ANML,
  title={The {ANML} Language},
  author={David E. Smith and Jeremy Frank and William Cushing},
  year={2008},
  booktitle={The {ICAPS}-08 Workshop on Knowledge Engineering for Planning and Scheduling},
  month=sep,
}


@InProceedings{2021NetHack,
  title = 	 {Insights From the NeurIPS 2021 NetHack Challenge},
  author =       {Hambro, Eric and Mohanty, Sharada and Babaev, Dmitrii and Byeon, Minwoo and Chakraborty, Dipam and Grefenstette, Edward and Jiang, Minqi and Daejin, Jo and Kanervisto, Anssi and Kim, Jongmin and Kim, Sungwoong and Kirk, Robert and Kurin, Vitaly and K{\"u}ttler, Heinrich and Kwon, Taehwon and Lee, Donghoon and Mella, Vegard and Nardelli, Nantas and Nazarov, Ivan and Ovsov, Nikita and Holder, Jack and Raileanu, Roberta and Ramanauskas, Karolis and Rockt{\"a}schel, Tim and Rothermel, Danielle and Samvelyan, Mikayel and Sorokin, Dmitry and Sypetkowski, Maciej and Sypetkowski, Micha\l{}},
  booktitle = 	 {Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track},
  pages = 	 {41--52},
  year = 	 {2022},
  editor = 	 {Kiela, Douwe and Ciccone, Marco and Caputo, Barbara},
  volume = 	 {176},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 dec,
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v176/hambro22a/hambro22a.pdf},
  url = 	 {https://proceedings.mlr.press/v176/hambro22a.html},
  abstract = 	 {In this report, we summarize the takeaways from the first NeurIPS 2021 NetHack Challenge. Participants were tasked with developing a program or agent that can win (i.e., ’ascend’ in) the popular dungeon-crawler game of NetHack by interacting with the NetHack Learning Environment (NLE), a scalable, procedurally generated, and challenging Gym environment for reinforcement learning (RL). The challenge showcased community-driven progress in AI with many diverse approaches significantly beating the previously best results on NetHack. Furthermore, it served as a direct comparison between neural (e.g., deep RL) and symbolic AI, as well as hybrid systems, demonstrating that on NetHack symbolic bots currently outperform deep RL by a large margin. Lastly, no agent got close to winning the game, illustrating NetHack’s suitability as a long-term benchmark for AI research.},
  doi={10.48550/arxiv.2203.11889}
}

@article{10.1098/rstb.2013.0480,
  title={Model-based hierarchical reinforcement learning and human action control},
  author={Botvinick, Matthew and Weinstein, Aaron},
  journal={Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume={369},
  number={1655},
  pages={20130480},
  year={2014},
  publisher={The Royal Society},
  doi={10.1098/rstb.2013.0480},
}

@inproceedings{bacon2017option,
  author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  title = {The option-critic architecture},
  year = {2017},
  publisher = {AAAI Press},
  abstract = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup \& Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
  booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
  pages = {1726–1734},
  numpages = {9},
  location = {San Francisco, California, USA},
  series = {AAAI'17}
}

@inproceedings{heess2016learning,
  title={Learning continuous control policies by stochastic value gradients},
  author={Heess, Nicolas and Wayne, Greg and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  booktitle={Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  volume={28},
  pages={2944--2952},
  isbn = {978-1-5108-2502-4},
  year={2015},
  doi={10.48550/arxiv.1510.09142}
}
@inproceedings{nachum2018data,
  title={Data-efficient hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  booktitle={Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  volume={31},
  pages={3303--3313},
  isbn={978-1-5108-8447-2},
  year={2018},
  doi={10.48550/arxiv.1805.08296}
}

@article{Chollet2019OnTM,
  title={On the Measure of Intelligence},
  author={François Chollet},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.01547},
  doi={10.48550/arXiv.1911.01547},
}
